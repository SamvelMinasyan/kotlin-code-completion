{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7df69c",
   "metadata": {},
   "source": [
    "# Preparing Kotlin code completion dataset and finetuning the Phi 1.5 model using PEFT(Parameter-Efficient Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae6957",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbcafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2fa0e4",
   "metadata": {},
   "source": [
    "### Preparing Kotlin files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments_and_format_new_lines(source_code):\n",
    "    # Remove single line comments and the newline character immediately following it\n",
    "    source_code = re.sub(r'//.*?\\n', '\\n', source_code)\n",
    "    # Remove multi-line comments\n",
    "    source_code = re.sub(r'/\\*.*?\\*/', '', source_code, flags=re.DOTALL)\n",
    "    # Reduce three or more consecutive newlines to exactly two newlines\n",
    "    source_code = re.sub(r'\\n{3,}', '\\n\\n', source_code)\n",
    "    return source_code\n",
    "\n",
    "def load_and_clean_files(directory_path):\n",
    "    all_files = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith('.kt'):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                cleaned_content = remove_comments_and_format_new_lines(content)\n",
    "                all_files.append(cleaned_content)\n",
    "    return all_files\n",
    "\n",
    "def split_dataset(data, train_ratio=0.6, val_ratio=0.2):\n",
    "    random.shuffle(data)\n",
    "    total = len(data)\n",
    "    train_end = int(total * train_ratio)\n",
    "    val_end = int(total * (train_ratio + val_ratio))\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Load and clean the data\n",
    "directory_path = 'kotlin_files'\n",
    "data = load_and_clean_files(directory_path)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_data, val_data, test_data = split_dataset(data)\n",
    "\n",
    "with open('train_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(f\"{item}\\n\\n\")\n",
    "\n",
    "with open('val_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in val_data:\n",
    "        f.write(f\"{item}\\n\\n\")\n",
    "\n",
    "with open('test_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in test_data:\n",
    "        f.write(f\"{item}\\n\\n\")\n",
    "\n",
    "print(\"Data preparation complete. Data split into training, validation, and test sets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ddbe5c",
   "metadata": {},
   "source": [
    "### Load a pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5868a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7df019",
   "metadata": {},
   "source": [
    "### Configure model quantization and Parameter Efficient Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-1_5\",\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebd3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c6071f",
   "metadata": {},
   "source": [
    "### Apply PEFT to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89138cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072c61f",
   "metadata": {},
   "source": [
    "### Load the dataset and prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a599ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = \"\"\n",
    "with open(\"val_data.txt\", \"r\") as f:\n",
    "    val_data = f.read()\n",
    "    \n",
    "chunk_size = 2000\n",
    "text_chunks = [val_data[i:i + chunk_size] for i in range(0, len(val_data), chunk_size)]\n",
    "\n",
    "val_df = pd.DataFrame(text_chunks, columns=['text'])\n",
    "val_df['Prompt'] = val_df['text'].str[:1000]\n",
    "val_df['Completion'] = val_df['text'].str[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ffca2",
   "metadata": {},
   "source": [
    "### Create a combined text field for tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df[\"text\"] = val_df[[\"Prompt\", \"Completion\"]].apply(lambda x: \"Prompt: \" + x[\"Prompt\"] + \" Completion: \" + x[\"Completion\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef505b4d",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    tokenized_text =  tokenizer(sample[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e377ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset.from_pandas(val_df)\n",
    "\n",
    "tokenized_data = data.map(tokenize, batched=True, desc=\"Tokenizing data\", remove_columns=data.column_names)\n",
    "\n",
    "# Split the tokenized data into training and test sets\n",
    "dataset = tokenized_data.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ccb24f",
   "metadata": {},
   "source": [
    "### Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"phi-1_5-finetuned-med-text\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        max_steps=1000,\n",
    "        num_train_epochs=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9454a134",
   "metadata": {},
   "source": [
    "### Function to compute metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f15f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    # Logic is missing because when I tried to run the evaluation with compute_metrics I was running into \"OutOfMemoryError: CUDA out of memory\" error\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": sentence_bleu,\n",
    "        \"ROUGE-L\": Rouge().get_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327281c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8cab6",
   "metadata": {},
   "source": [
    "### Initialize and run the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ce2b36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-722841c3b220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m trainer = Trainer(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_arguments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=training_arguments,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "#     compute_metrics=compute_metrics\n",
    ")\n",
    "a = trainer.evaluate(eval_dataset)\n",
    "print(a)\n",
    "trainer.train()\n",
    "b = trainer.evaluate(eval_dataset)\n",
    "print(b)\n",
    "\n",
    "# Training loss is decreasing but I was not able to compute all metrics because of \"OutOfMemoryError: CUDA out of memory\" issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ad4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccb9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
